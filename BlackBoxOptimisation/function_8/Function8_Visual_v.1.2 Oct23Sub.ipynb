{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "l87YfqG-ua5r"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.13/site-packages/sklearn/gaussian_process/kernels.py:452: ConvergenceWarning: The optimal value found for dimension 5 of parameter k1__k2__length_scale is close to the specified upper bound 1e+50. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.13/site-packages/sklearn/gaussian_process/kernels.py:452: ConvergenceWarning: The optimal value found for dimension 7 of parameter k1__k2__length_scale is close to the specified upper bound 1e+50. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original candidates= (500, 8)\n",
      "Reduced candidates to 241 promising ones using SVM.\n",
      "Reduced candidates= (241, 8)\n",
      "first point ei_vals[ 237 ]= 0.008448734808565436\n",
      "seed_points=\n",
      " [[0.04080233 0.60917259 0.01175471 0.07830876 0.58317929 0.62059085\n",
      "  0.17053724 0.40322938]\n",
      " [0.09265999 0.32537231 0.13255613 0.33445081 0.76733688 0.17134511\n",
      "  0.07414239 0.80773506]\n",
      " [0.1996495  0.40188013 0.11413416 0.56561133 0.72265224 0.68763408\n",
      "  0.00718758 0.9824687 ]\n",
      " [0.19764459 0.03596475 0.00927308 0.36674591 0.19362442 0.50362833\n",
      "  0.13141981 0.62675898]\n",
      " [0.24938085 0.3142001  0.18716287 0.22048043 0.66200035 0.2217289\n",
      "  0.18884683 0.27084396]\n",
      " [0.71516389 0.08387129 0.05634294 0.03039783 0.10683546 0.25548415\n",
      "  0.04838679 0.59644634]\n",
      " [0.36269686 0.76078878 0.02648455 0.44681295 0.37185457 0.47707401\n",
      "  0.12762069 0.22250687]\n",
      " [0.3642252  0.52999352 0.0039646  0.1307006  0.2790991  0.04683844\n",
      "  0.23823314 0.42109562]\n",
      " [0.50230029 0.03761517 0.10244131 0.52392339 0.8560083  0.43263456\n",
      "  0.00372936 0.21230311]\n",
      " [0.03687728 0.70421875 0.31575293 0.02268245 0.91261053 0.23637586\n",
      "  0.0934657  0.73823609]]\n",
      "seed=\t [0.04080233 0.60917259 0.01175471 0.07830876 0.58317929 0.62059085\n",
      " 0.17053724 0.40322938] \tEI= 0.07267829808425458\n",
      "best_x=\t [0.08472078 0.31830431 0.         0.2393278  1.         0.62059085\n",
      " 0.14189608 0.40322938] \tEI= 0.07267829808425458\n",
      "seed=\t [0.09265999 0.32537231 0.13255613 0.33445081 0.76733688 0.17134511\n",
      " 0.07414239 0.80773506] \tEI= 0.07267829819397466\n",
      "best_x=\t [0.08472619 0.31830945 0.         0.23932889 1.         0.17134511\n",
      " 0.14190905 0.80773506] \tEI= 0.07267829819397466\n",
      "seed=\t [0.1996495  0.40188013 0.11413416 0.56561133 0.72265224 0.68763408\n",
      " 0.00718758 0.9824687 ] \tEI= 0.07267829759331229\n",
      "seed=\t [0.19764459 0.03596475 0.00927308 0.36674591 0.19362442 0.50362833\n",
      " 0.13141981 0.62675898] \tEI= 0.07267829808704475\n",
      "seed=\t [0.24938085 0.3142001  0.18716287 0.22048043 0.66200035 0.2217289\n",
      " 0.18884683 0.27084396] \tEI= 0.07267829798085322\n",
      "seed=\t [0.71516389 0.08387129 0.05634294 0.03039783 0.10683546 0.25548415\n",
      " 0.04838679 0.59644634] \tEI= 0.07267829807877581\n",
      "seed=\t [0.36269686 0.76078878 0.02648455 0.44681295 0.37185457 0.47707401\n",
      " 0.12762069 0.22250687] \tEI= 0.07267829755322086\n",
      "seed=\t [0.3642252  0.52999352 0.0039646  0.1307006  0.2790991  0.04683844\n",
      " 0.23823314 0.42109562] \tEI= 0.07267829802978915\n",
      "seed=\t [0.50230029 0.03761517 0.10244131 0.52392339 0.8560083  0.43263456\n",
      " 0.00372936 0.21230311] \tEI= 0.07267829785210281\n",
      "seed=\t [0.03687728 0.70421875 0.31575293 0.02268245 0.91261053 0.23637586\n",
      " 0.0934657  0.73823609] \tEI= 0.072678298072287\n",
      "Suggested next query point (raw input space): 0.084726-0.318309-0.000000-0.239329-1.000000-0.171345-0.141909-0.807735\n",
      "Expected Improvement at this point (normalized): 0.07267829819397466\n",
      "GP predicted mean at this point (original y scale): [9.98108908]\n",
      "GP predicted stddev at this point (original y scale): [0.15109154]\n",
      "This visualization works only for 2D input data.\n",
      "SVM visualisation works only for 2-D inputs.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import Matern, ConstantKernel as C, WhiteKernel\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from scipy.stats import norm\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Load initial data\n",
    "# ------------------------------------------------------------\n",
    "X = np.load(\"initial_inputs.npy\")        # shape (n0, d)\n",
    "y = np.load(\"initial_outputs.npy\")       # shape (n0,)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Append new information\n",
    "# ------------------------------------------------------------\n",
    "X = np.append(X,[[0.060280, 0.000000, 0.134972, 0.000000, 1.000000, 0.404336, 0.057764, 0.516640]], axis=0)  # Append week1 inputs\n",
    "y = np.append(y, 9.8814618316)         # Append week1 outputs\n",
    "\n",
    "X = np.append(X,[[0.122641, 0.153980, 0.162409, 0.045605, 1.000000, 0.536662, 0.260830, 0.252219]], axis=0)  # Append week2 inputs\n",
    "y = np.append(y, 9.9440697415299)          # Append week2 outputs\n",
    "\n",
    "# Save the updated data\n",
    "np.save(\"updated_inputs_PW2.npy\", X)\n",
    "np.save(\"updated_outputs_PW2.npy\", y)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Normalise inputs and outputs\n",
    "# ------------------------------------------------------------\n",
    "x_scaler = StandardScaler()\n",
    "Xn = x_scaler.fit_transform(X)\n",
    "\n",
    "y_mean = y.mean()\n",
    "y_std = y.std() if y.std() > 0 else 1.0\n",
    "yn = (y - y_mean) / y_std                 # GP works better with normalized target\n",
    "\n",
    "\n",
    "def visualize_iteration(Xn, seed_points, suggested_point):\n",
    "    \"\"\"\n",
    "    Simple 2D visualization of current iteration:\n",
    "    - Xn: existing normalized input data (n x 2)\n",
    "    - seed_points: candidate points used for local optimization (m x 2)\n",
    "    - suggested_point: final chosen query point (1 x 2)\n",
    "    \"\"\"\n",
    "    if Xn.shape[1] != 2:\n",
    "        print(\"This visualization works only for 2D input data.\")\n",
    "        return\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(Xn[:, 0], Xn[:, 1], c='blue', label='Existing points', s=40, alpha=0.7)\n",
    "    plt.scatter(seed_points[:, 0], seed_points[:, 1], c='orange', label='Seed points', s=50, alpha=0.7, edgecolors='k')\n",
    "    plt.scatter(suggested_point[0], suggested_point[1], c='red', label='Suggested point', s=120, marker='*', edgecolors='k')\n",
    "\n",
    "    plt.title(\"Bayesian Optimization Progress (2D)\")\n",
    "    plt.xlabel(\"Feature 1 (normalized)\")\n",
    "    plt.ylabel(\"Feature 2 (normalized)\")\n",
    "    #plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def visualize_svm(Xn, yn, candidates_raw, good_candidates_raw,\n",
    "                  suggested_point, svm_clf, x_scaler):\n",
    "    \"\"\"\n",
    "    Plot the SVM decision boundary together with:\n",
    "      • all random candidates (gray)\n",
    "      • candidates kept by SVM (orange)\n",
    "      • existing data (blue)\n",
    "      • final suggestion (red star)\n",
    "    \"\"\"\n",
    "    if Xn.shape[1] != 2:\n",
    "        print(\"SVM visualisation works only for 2-D inputs.\")\n",
    "        return\n",
    "\n",
    "    # Create a dense grid for the contour\n",
    "    h = 0.01\n",
    "    x_min, x_max = Xn[:,0].min()-0.2, Xn[:,0].max()+0.2\n",
    "    y_min, y_max = Xn[:,1].min()-0.2, Xn[:,1].max()+0.2\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "    grid = np.c_[xx.ravel(), yy.ravel()]\n",
    "    Z = svm_clf.decision_function(grid)\n",
    "    Z = Z.reshape(xx.shape)\n",
    "\n",
    "    plt.figure(figsize=(9,7))\n",
    "    # Decision boundary\n",
    "    plt.contourf(xx, yy, Z, levels=[-np.inf,0,np.inf],\n",
    "                 colors=['#FFDDDD','#DDFFDD'], alpha=0.3)\n",
    "    plt.contour(xx, yy, Z, levels=[0], linewidths=2, colors='k')\n",
    "\n",
    "    # All random candidates\n",
    "    plt.scatter(candidates_raw[:,0], candidates_raw[:,1],\n",
    "                c='lightgray', s=15, alpha=0.6, label='All random candidates')\n",
    "\n",
    "    # Candidates kept by SVM\n",
    "    plt.scatter(good_candidates_raw[:,0], good_candidates_raw[:,1],\n",
    "                c='orange', s=40, edgecolor='k', label='SVM-filtered candidates')\n",
    "\n",
    "    # Existing points (colour-coded by class for reference)\n",
    "    median_y = np.median(yn)\n",
    "    colors = np.where(yn >= median_y, 'blue', 'cyan')\n",
    "    plt.scatter(Xn[:,0], Xn[:,1], c=colors, s=80,\n",
    "                edgecolor='k', label='Existing points (≥/ < median)')\n",
    "\n",
    "    # Suggested point\n",
    "    plt.scatter(suggested_point[0], suggested_point[1], c='red',\n",
    "                s=150, marker='*', edgecolor='k', label='Suggested point')\n",
    "\n",
    "    plt.xlim(x_min, x_max)\n",
    "    plt.ylim(y_min, y_max)\n",
    "    plt.title(\"SVM Decision Boundary + Candidate Filtering\")\n",
    "    plt.xlabel(\"Feature 1 (norm.)\")\n",
    "    plt.ylabel(\"Feature 2 (norm.)\")\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Fit a Gaussian Process surrogate\n",
    "# ------------------------------------------------------------\n",
    "d = X.shape[1]\n",
    "kernel = C(1.0, (1e-3, 1e3)) * Matern(length_scale=np.ones(d),\n",
    "                                      length_scale_bounds=(1e-5, 1e50),\n",
    "                                      nu=2.5)\n",
    "kernel += WhiteKernel(noise_level=1e-6,\n",
    "                      noise_level_bounds=(1e-15, 1e10))\n",
    "\n",
    "gp = GaussianProcessRegressor(kernel=kernel,\n",
    "                              normalize_y=False,\n",
    "                              n_restarts_optimizer=10,\n",
    "                              random_state=0)\n",
    "gp.fit(Xn, yn)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Add SVM to classify promising regions\n",
    "# ------------------------------------------------------------\n",
    "if len(yn) > 1:  # need at least two points for classification\n",
    "    median_y = np.median(yn)\n",
    "    labels = (yn >= median_y).astype(int)\n",
    "    svm_clf = SVC(kernel='rbf', C=1.0)\n",
    "    svm_clf.fit(Xn, labels)\n",
    "else:\n",
    "    svm_clf = None\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Define Expected Improvement acquisition (for maximization)\n",
    "# ------------------------------------------------------------\n",
    "f_best = yn.max()\n",
    "xi = 0.01     # exploration parameter\n",
    "\n",
    "def predict_raw(x_raw):\n",
    "    \"\"\"Predict mean and std in normalized GP space for a raw input x_raw.\"\"\"\n",
    "    x = np.atleast_2d(x_raw)\n",
    "    xn = x_scaler.transform(x)\n",
    "    mu, sigma = gp.predict(xn, return_std=True)\n",
    "    return mu.ravel(), sigma.ravel()\n",
    "\n",
    "def expected_improvement_raw(x_raw, xi=xi):\n",
    "    \"\"\"Compute EI for a raw input x_raw.\"\"\"\n",
    "    mu, sigma = predict_raw(x_raw)\n",
    "    sigma = np.maximum(sigma, 1e-9)       # avoid division by zero\n",
    "    z = (mu - f_best - xi) / sigma\n",
    "    ei = (mu - f_best - xi) * norm.cdf(z) + sigma * norm.pdf(z)\n",
    "    #print (\"x_raw=\", x_raw, \"mu=\", mu, \"sigma=\", sigma,\"ei=\",ei)\n",
    "    return ei.ravel()[0]\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Search for the next query point\n",
    "# ------------------------------------------------------------\n",
    "# Domain is assumed to be [0,1]^d (based on inspection of initial data)\n",
    "bounds = [(0.0, 1.0)] * d\n",
    "\n",
    "# Global random search to find good seeds\n",
    "n_seeds = 500\n",
    "rng = np.random.default_rng(1)\n",
    "# Need to decide where the random points needs to be\n",
    "candidates_raw = rng.uniform(0.0, 1.0, size=(n_seeds, d))\n",
    "\n",
    "\n",
    "print (\"Original candidates=\", candidates_raw.shape)\n",
    "good_candidates_raw = candidates_raw.copy()\n",
    "\n",
    "if svm_clf is not None:\n",
    "    candidates_n = x_scaler.transform(candidates_raw)\n",
    "    predicted_labels = svm_clf.predict(candidates_n)\n",
    "    good_idx = np.where(predicted_labels == 1)[0]\n",
    "    if len(good_idx) > 0:\n",
    "        good_candidates_raw = candidates_raw[good_idx]\n",
    "        print(f\"Reduced candidates to {len(good_candidates_raw)} promising ones using SVM.\")\n",
    "\n",
    "print (\"Reduced candidates=\", good_candidates_raw.shape)\n",
    "\n",
    "# ---- EI evaluation on the (possibly reduced) candidate set ----\n",
    "ei_vals = np.array([expected_improvement_raw(c) for c in good_candidates_raw])\n",
    "best_idx = np.argmax(ei_vals)\n",
    "print (\"first point ei_vals[\",best_idx,\"]=\", ei_vals[best_idx])\n",
    "\n",
    "# ---- Take top-10 seeds for local optimisation ----\n",
    "seed_points = good_candidates_raw[np.argsort(-ei_vals)[:10]]\n",
    "print (\"seed_points=\\n\", seed_points)\n",
    "\n",
    "best_x = None\n",
    "best_val = -1.0\n",
    "for s in seed_points:\n",
    "    res = minimize(lambda xx: -expected_improvement_raw(xx),\n",
    "                   x0=s,\n",
    "                   bounds=bounds,\n",
    "                   method=\"L-BFGS-B\",\n",
    "                   options={'maxiter':300})\n",
    "    if res.success:\n",
    "        val = -res.fun\n",
    "        print ( \"seed=\\t\", s, \"\\tEI=\", val)\n",
    "        #visualize_iteration(Xn, seed_points, best_x)\n",
    "        if val > best_val:\n",
    "            best_val = val\n",
    "            best_x = res.x.copy()\n",
    "            print ( \"best_x=\\t\", best_x, \"\\tEI=\", val )\n",
    "        \n",
    "        \n",
    "\n",
    "# Fallback if optimizer fails\n",
    "if best_x is None:\n",
    "    best_x = good_candidates_raw[best_idx]\n",
    "    best_val = ei_vals[best_idx]\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Report results\n",
    "# ------------------------------------------------------------\n",
    "suggested_point = np.atleast_1d(best_x)\n",
    "mu_s, sigma_s = predict_raw(suggested_point.reshape(1, -1))\n",
    "\n",
    "# Convert mean/std back to original y-scale\n",
    "mu_orig = mu_s * y_std + y_mean\n",
    "sigma_orig = sigma_s * y_std\n",
    "next_query = \"-\".join([f\"{xi:.6f}\" for xi in suggested_point])\n",
    "\n",
    "print(\"Suggested next query point (raw input space):\", next_query)\n",
    "print(\"Expected Improvement at this point (normalized):\", best_val)\n",
    "print(\"GP predicted mean at this point (original y scale):\", mu_orig)\n",
    "print(\"GP predicted stddev at this point (original y scale):\", sigma_orig)\n",
    "\n",
    "# Visualise 1. Standard BO progress plot\n",
    "visualize_iteration(X, seed_points, best_x)\n",
    "\n",
    "# 2. SVM decision-boundary plot (only when SVM was trained)\n",
    "if svm_clf is not None:\n",
    "    visualize_svm(X, y,\n",
    "                  candidates_raw,\n",
    "                  good_candidates_raw,\n",
    "                  best_x,\n",
    "                  svm_clf,\n",
    "                  x_scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
